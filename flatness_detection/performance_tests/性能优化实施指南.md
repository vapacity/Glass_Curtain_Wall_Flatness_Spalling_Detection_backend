# 性能优化实施指南

## 快速优化方案（可立即实施）

### 1. OSS下载优化

#### 1.1 实现本地缓存
```python
# 在app.py中添加缓存机制
import hashlib
from functools import lru_cache

# 图片缓存目录
CACHE_DIR = "./image_cache"
os.makedirs(CACHE_DIR, exist_ok=True)

@lru_cache(maxsize=100)
def download_image_with_cache(url):
    """带缓存的图片下载"""
    # 生成URL的hash作为缓存key
    url_hash = hashlib.md5(url.encode()).hexdigest()
    cache_path = os.path.join(CACHE_DIR, f"{url_hash}.jpg")
    
    # 检查缓存
    if os.path.exists(cache_path):
        print(f"使用缓存图片: {cache_path}")
        return cache_path
    
    # 下载并缓存
    image_path = downloadImage(url)
    if image_path:
        shutil.copy(image_path, cache_path)
    
    return cache_path
```

#### 1.2 并行下载优化
```python
import concurrent.futures

def download_images_parallel(urls, max_workers=5):
    """并行下载多张图片"""
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_url = {executor.submit(download_image_with_cache, url): url for url in urls}
        results = {}
        
        for future in concurrent.futures.as_completed(future_to_url):
            url = future_to_url[future]
            try:
                results[url] = future.result()
            except Exception as e:
                print(f"下载失败 {url}: {e}")
                results[url] = None
                
    return results
```

### 2. 模型推理优化

#### 2.1 启用半精度推理
```python
# 在模型加载部分添加
def load_model_optimized():
    model = GDNet().to(device)
    model.load_state_dict(torch.load('./model/200.pth', map_location=device))
    
    # 如果是GPU，启用半精度
    if device.type == 'cuda':
        model = model.half()  # FP16
        
    model.eval()
    return model

# 推理时也使用半精度
def inference_optimized(model, img_var):
    with torch.no_grad():
        if device.type == 'cuda':
            img_var = img_var.half()
        
        # 启用cudnn优化
        torch.backends.cudnn.benchmark = True
        
        f1, f2, f3 = model(img_var)
    return f1, f2, f3
```

#### 2.2 批处理实现
```python
class BatchProcessor:
    def __init__(self, model, batch_size=4, timeout=2.0):
        self.model = model
        self.batch_size = batch_size
        self.timeout = timeout
        self.queue = []
        self.results = {}
        
    def add_request(self, request_id, image):
        self.queue.append((request_id, image))
        
        # 如果达到批大小或超时，处理批次
        if len(self.queue) >= self.batch_size:
            self.process_batch()
            
    def process_batch(self):
        if not self.queue:
            return
            
        batch_ids = []
        batch_images = []
        
        for req_id, img in self.queue[:self.batch_size]:
            batch_ids.append(req_id)
            batch_images.append(img)
            
        # 批量推理
        batch_tensor = torch.stack(batch_images)
        with torch.no_grad():
            results = self.model(batch_tensor)
            
        # 保存结果
        for i, req_id in enumerate(batch_ids):
            self.results[req_id] = results[i]
            
        # 清理已处理的请求
        self.queue = self.queue[self.batch_size:]
```

### 3. 数据库优化

#### 3.1 添加索引
```sql
-- 为常用查询添加索引
ALTER TABLE historyData ADD INDEX idx_user_timestamp (user_name, timestamp);
ALTER TABLE outputInfo ADD INDEX idx_output_id (output_id);
```

#### 3.2 连接池实现
```python
from dbutils.pooled_db import PooledDB

# 创建数据库连接池
db_pool = PooledDB(
    creator=pymysql,
    maxconnections=20,
    mincached=5,
    maxcached=10,
    blocking=True,
    host='1.117.69.116',
    user='flatness',
    password='TXaRZbA4mHwPsPch',
    database='flatness',
    charset='utf8mb4'
)

def get_db_connection():
    """从连接池获取连接"""
    return db_pool.connection()
```

### 4. 前端优化

#### 4.1 图片懒加载
```javascript
// 使用Intersection Observer实现懒加载
const imageObserver = new IntersectionObserver((entries, observer) => {
    entries.forEach(entry => {
        if (entry.isIntersecting) {
            const img = entry.target;
            img.src = img.dataset.src;
            img.classList.remove('lazy');
            observer.unobserve(img);
        }
    });
});

// 应用到所有懒加载图片
document.querySelectorAll('img.lazy').forEach(img => {
    imageObserver.observe(img);
});
```

#### 4.2 加载进度提示
```javascript
// 图片加载进度
function loadImageWithProgress(url, progressCallback) {
    return new Promise((resolve, reject) => {
        const xhr = new XMLHttpRequest();
        xhr.open('GET', url, true);
        xhr.responseType = 'blob';
        
        xhr.onprogress = (e) => {
            if (e.lengthComputable) {
                const percentComplete = (e.loaded / e.total) * 100;
                progressCallback(percentComplete);
            }
        };
        
        xhr.onload = () => {
            if (xhr.status === 200) {
                const blob = xhr.response;
                const imgUrl = URL.createObjectURL(blob);
                resolve(imgUrl);
            } else {
                reject(new Error('加载失败'));
            }
        };
        
        xhr.send();
    });
}
```

### 5. Redis缓存集成

#### 5.1 安装和配置
```bash
pip install redis
```

#### 5.2 缓存实现
```python
import redis
import json

# Redis连接
redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)

def cache_result(key, data, expire=3600):
    """缓存结果"""
    redis_client.setex(key, expire, json.dumps(data))
    
def get_cached_result(key):
    """获取缓存结果"""
    data = redis_client.get(key)
    return json.loads(data) if data else None

# 在检测接口中使用缓存
@app.route('/flatness/detect', methods=['POST'])
def detect():
    data = request.get_json()
    url = data.get('url')
    
    # 生成缓存key
    cache_key = f"flatness:{hashlib.md5(url.encode()).hexdigest()}"
    
    # 检查缓存
    cached = get_cached_result(cache_key)
    if cached:
        return jsonify(cached), 200
    
    # 执行检测...
    result = perform_detection(url)
    
    # 缓存结果
    cache_result(cache_key, result)
    
    return jsonify(result), 200
```

### 6. 监控和日志

#### 6.1 性能监控
```python
import time
from functools import wraps

def monitor_performance(func):
    """性能监控装饰器"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        
        result = func(*args, **kwargs)
        
        end_time = time.time()
        duration = end_time - start_time
        
        # 记录性能数据
        app.logger.info(f"{func.__name__} 执行时间: {duration:.2f}秒")
        
        # 如果执行时间过长，发出警告
        if duration > 5:
            app.logger.warning(f"{func.__name__} 执行时间过长: {duration:.2f}秒")
            
        return result
    return wrapper

# 应用到关键函数
@monitor_performance
def detect_glass_flatness(image_path):
    # 检测逻辑
    pass
```

### 7. 部署优化

#### 7.1 使用Gunicorn多进程
```bash
# 安装gunicorn
pip install gunicorn

# 启动服务（4个worker进程）
gunicorn -w 4 -b 0.0.0.0:8080 app:app --timeout 120
```

#### 7.2 Nginx配置
```nginx
# nginx.conf
upstream flatness_backend {
    server 127.0.0.1:8080;
    server 127.0.0.1:8081;
    server 127.0.0.1:8082;
    server 127.0.0.1:8083;
}

server {
    listen 80;
    
    # 启用gzip压缩
    gzip on;
    gzip_types text/plain application/json image/jpeg;
    
    # 静态文件缓存
    location /static/ {
        expires 1d;
        add_header Cache-Control "public";
    }
    
    # API代理
    location /flatness/ {
        proxy_pass http://flatness_backend;
        proxy_set_header Host $host;
        proxy_read_timeout 120s;
    }
}
```

## 实施优先级

1. **立即实施**（1-2天）
   - 本地图片缓存
   - Redis结果缓存
   - 数据库索引优化

2. **短期实施**（1周）
   - 模型半精度推理
   - 前端懒加载
   - Gunicorn多进程部署

3. **中期实施**（2-4周）
   - CDN集成
   - 批处理优化
   - 完整监控体系

## 性能测试验证

实施优化后，使用以下命令验证性能提升：

```bash
# 1. 基准测试
cd performance_tests
python benchmark_test.py

# 2. 压力测试
locust -f performance_test.py --headless --users 50 --spawn-rate 5 --run-time 5m

# 3. 对比优化前后的响应时间
python simple_performance_test.py --system all
```

预期效果：
- 缓存命中时响应时间降低80%
- 平均响应时间降低50%
- 支持并发数提升3倍